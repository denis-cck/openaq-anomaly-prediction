{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22074dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b338e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from openaq_anomaly_prediction.config import Configuration as config\n",
    "from openaq_anomaly_prediction.utils.logging import logger, ProgressLogger\n",
    "from openaq_anomaly_prediction.load.openaq import client as openaq, AreaDownloader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1e9cc",
   "metadata": {},
   "source": [
    "### Get Area Data (New Delhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4dfaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;102;102;102m01:05:54\u001b[0m \u001b[1m    INFO \u001b[1m|\u001b[0m\u001b[1m  \u001b[0m\u001b[37mArea: NEW_DELHI\u001b[0m\n",
      "\u001b[38;2;102;102;102m01:05:54\u001b[0m \u001b[1m    INFO \u001b[1m|\u001b[0m\u001b[1m  \u001b[0m\u001b[37mLocations: (107, 27)\u001b[0m\n",
      "\u001b[38;2;102;102;102m01:05:54\u001b[0m \u001b[1m    INFO \u001b[1m|\u001b[0m\u001b[1m  \u001b[0m\u001b[37mSensors: (1241, 7)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_delhi = AreaDownloader(area_name=\"new_delhi\")\n",
    "new_delhi.load_bbox(76.772461, 28.161110, 77.768372, 28.943516)\n",
    "\n",
    "logger.info(f\"Area: {new_delhi.area_name.upper()}\")\n",
    "logger.info(f\"Locations: {new_delhi.locations.shape}\")\n",
    "logger.info(f\"Sensors: {new_delhi.sensors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f8b12",
   "metadata": {},
   "source": [
    "### Create STG file (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062955fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/deniscck/code/denis-cck/openaq_anomaly_prediction/data/csv/new_delhi_2024_2024-01-01_2024-03-31.csv',\n",
      " '/home/deniscck/code/denis-cck/openaq_anomaly_prediction/data/csv/new_delhi_2024_2024-07-01_2024-09-30.csv',\n",
      " '/home/deniscck/code/denis-cck/openaq_anomaly_prediction/data/csv/new_delhi_2024_2024-04-01_2024-06-30.csv',\n",
      " '/home/deniscck/code/denis-cck/openaq_anomaly_prediction/data/csv/new_delhi_2024_2024-10-01_2024-12-31.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from openaq_anomaly_prediction.utils.helpers import concat_csv_to_csv\n",
    "import os\n",
    "# CONCATENATE: Concatenate several trimesters CSVs into a single CSV file\n",
    "\n",
    "staging_csv_prefix = \"new_delhi_2024\"\n",
    "\n",
    "selected_files = glob.glob(os.path.join(config.DATA_CSV_PATH, f\"{staging_csv_prefix}*.raw.csv\"))\n",
    "pprint(selected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[38;2;102;102;102m01:06:21      4/4 |   ó°˜ \u001b[0m\u001b[1m\u001b[38;2;43;214;131m100%\u001b[0m\u001b[38;2;102;102;102m: \u001b[0m\u001b[38;2;102;102;102mAppending CSV files to final CSV -> data/csv/new_delhi_2024.stg.csv\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# concat_csv_to_csv(selected_files, f\"{staging_csv_prefix}.stg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb41c23",
   "metadata": {},
   "source": [
    "### Clean DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0918bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_measurements(df: pd.DataFrame, area: AreaDownloader) -> pd.DataFrame:\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # CLEAN MEASUREMENTS DATAFRAME\n",
    "    # print(f\"{'-' * 44}\\nCLEANING [MEASUREMENTS] DATAFRAME:\")\n",
    "    clean_measurements = df[\n",
    "        [\n",
    "            \"sensor_id\",\n",
    "            \"value\",\n",
    "            \"parameter.id\",\n",
    "            \"parameter.name\",\n",
    "            \"parameter.units\",\n",
    "            \"period.datetimeFrom.local\",\n",
    "            \"period.datetimeTo.local\",\n",
    "            \"period.datetimeFrom.utc\",\n",
    "            \"period.datetimeTo.utc\",\n",
    "            \"coverage.expectedCount\",\n",
    "            \"coverage.observedCount\",\n",
    "        ]\n",
    "    ]\n",
    "    # display(clean_measurements.head(1))\n",
    "\n",
    "    # CLEAN SENSORS DATAFRAME\n",
    "    # print(f\"\\n{'-' * 44}\\nCLEANING [SENSORS] DATAFRAME:\")\n",
    "    clean_sensors = new_delhi.sensors[\n",
    "        [\n",
    "            \"location_id\",\n",
    "            \"id\",\n",
    "            \"name\",\n",
    "            \"parameter.displayName\",\n",
    "        ]\n",
    "    ]\n",
    "    # display(clean_sensors.head(1))\n",
    "\n",
    "    # CLEAN LOCATIONS DATAFRAME\n",
    "    # print(f\"\\n{'-' * 44}\\nCLEANING [LOCATIONS] DATAFRAME:\")\n",
    "    clean_locations = new_delhi.locations[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"name\",\n",
    "            \"isMobile\",\n",
    "            \"isMonitor\",  # Maybe it's whether it's recognized as an \"official\" monitoring station or not?\n",
    "            \"country.id\",\n",
    "            \"country.code\",\n",
    "            \"country.name\",\n",
    "            \"owner.id\",\n",
    "            \"owner.name\",\n",
    "            \"provider.id\",\n",
    "            \"provider.name\",\n",
    "            \"coordinates.latitude\",\n",
    "            \"coordinates.longitude\",\n",
    "\n",
    "            # KEEP BUT DON'T NEED FOR MEASUREMENTS\n",
    "            # \"datetimeFirst.local\",\n",
    "            # \"datetimeLast.local\",\n",
    "            \"datetimeFirst.utc\",\n",
    "            \"datetimeLast.utc\",\n",
    "\n",
    "            # CUSTOM FIELDS\n",
    "            # \"sensors_flat\",  # custom field added in AreaDownloader\n",
    "            # \"instruments_flat\",  # TODO: custom field added in AreaDownloader.\n",
    "            #    Not very standardized (sometimes duplicates) and no way of linking it to sensors/measurements.\n",
    "            #    You can only know which instruments are used in a location, but not which instrument is used for which sensor/parameter.\n",
    "            #    So for now we just keep it for reference but don't use it.\n",
    "\n",
    "            # EMPTY IN NEW DELHI\n",
    "            # \"locality\",  # TODO: ???: 106/107 empty in New Delhi\n",
    "            # \"bounds\",  # TODO: ???: all locations have fixed coordinates (no bounds just a point)\n",
    "            # \"distance\",  # TODO: ???: fully empty in New Delhi\n",
    "            # \"licenses\",  # TODO: ???: Vast majority of locations have NaN here, but there are some. Even then is that really useful? IDK just drop it\n",
    "\n",
    "            # DON'T KEEP\n",
    "            # \"instruments\",\n",
    "            # \"sensors\",\n",
    "            # \"timezone\",  # all the same usually for a city-sized area, and doesn't really influence the measurements themselves\n",
    "            # \"datetimeFirst\",  # NaT (not a time) for all locations in New Delhi\n",
    "            # \"datetimeLast\",  # NaT (not a time) for all locations in New Delhi\n",
    "        ]\n",
    "    ].rename(\n",
    "        columns={\n",
    "            \"id\": \"location_id\",\n",
    "            \"name\": \"location_name\",\n",
    "            \"datetimeFirst.utc\": \"location.datetimeFirst.utc\",\n",
    "            \"datetimeLast.utc\": \"location.datetimeLast.utc\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # JOIN THE MEASUREMENTS WITH THE SENSORS ON SENSOR_ID\n",
    "    df_joined = clean_measurements.join(clean_sensors.set_index(\"id\"), on=\"sensor_id\", how=\"left\")\n",
    "\n",
    "    # JOIN THE PREVIOUS RESULT WITH THE LOCATIONS ON LOCATION_ID\n",
    "    df_final = df_joined.join(clean_locations.set_index(\"location_id\"), on=\"location_id\", how=\"left\")\n",
    "\n",
    "    # REORDER COLUMNS\n",
    "    ordered_columns = [\n",
    "        \"location_id\",\n",
    "        \"sensor_id\",\n",
    "        \"name\",\n",
    "        \"value\",\n",
    "        \"parameter.id\",\n",
    "        \"parameter.name\",\n",
    "        \"parameter.units\",\n",
    "        \"parameter.displayName\",\n",
    "        \"period.datetimeFrom.local\",\n",
    "        \"period.datetimeTo.local\",\n",
    "        \"period.datetimeFrom.utc\",\n",
    "        \"period.datetimeTo.utc\",\n",
    "        \"location.datetimeFirst.utc\",\n",
    "        \"location.datetimeLast.utc\",\n",
    "        \"coordinates.latitude\",\n",
    "        \"coordinates.longitude\",\n",
    "        \"location_name\",\n",
    "        \"isMobile\",\n",
    "        \"isMonitor\",\n",
    "        \"country.id\",\n",
    "        \"country.code\",\n",
    "        \"country.name\",\n",
    "        \"owner.id\",\n",
    "        \"owner.name\",\n",
    "        \"provider.id\",\n",
    "        \"provider.name\",\n",
    "        \"coverage.expectedCount\",\n",
    "        \"coverage.observedCount\",\n",
    "    ]\n",
    "    df_final = df_final[ordered_columns]\n",
    "\n",
    "    # TODO: Move to a function and apply to all dataframes after loading/cleaning\n",
    "    # Convert columns with \"datetime\" in their names to datetime types\n",
    "    datetime_columns = [col for col in df_final.columns if \"datetime\" in col]\n",
    "    for col in datetime_columns:\n",
    "        df_final[col] = pd.to_datetime(df_final[col])\n",
    "\n",
    "    # display(df_final.head(1))\n",
    "    # print(len(df_final.columns))\n",
    "    # print(f\"Final measurements dataframe memory usage: {df_final.memory_usage(index=True, deep=True).sum() / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3593a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_delhi_2024: (42107, 34)\n"
     ]
    }
   ],
   "source": [
    "staging_csv_path = os.path.join(config.DATA_CSV_PATH, f\"{staging_csv_prefix}.stg.csv\")\n",
    "\n",
    "df_stg = pd.read_csv(staging_csv_path)\n",
    "print(f\"{staging_csv_prefix}: {df_stg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b13ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int = get_clean_measurements(df_stg, new_delhi)\n",
    "df_int = new_delhi.get_clean_measurements(df_stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int.to_csv(os.path.join(config.DATA_CSV_PATH, f\"{staging_csv_prefix}_measurements.int.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1701a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.to_parquet(os.path.join(config.DATA_CSV_PATH, f\"{staging_csv_prefix}_measurements.int.parquet\"), index=False, compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
